[
    {
        "user": "U065TBUL5EE",
        "type": "message",
        "ts": "1716861712.545789",
        "client_msg_id": "BF7644B5-A4AD-4B56-BEA4-2339302318F2",
        "text": "\n\nLLM에서 왜 quantization이 중요해졌는지에 대해 GPU의 발전역사와 연관되어 자세하게 설명된 글이 있어 공유해봅니다!\n\n\n\n(5\/27 Microsoft 사내 채널 릴레이 연재글입니다)\n\nAI 가속기의 과거, 현재, 미래\n\n1.\n \nMicrosoft에서 제공하는 많은 AI 서비스의 중심에는 VM으로 대표되는 Azure AI 인프라스트럭처가 있습니다. 특히 일반적인 범용 컴퓨팅용 VM에 더해 AI 작업에 요구되는 특화된 연산성능을 강화한 가속기를 탑재한 포트폴리오도 점점 다양해지고 있지요. 역사적으로(?) 컴퓨팅 성능을 향상시키기 위한 트렌드는 파편화된 연산장치를 하나로 통합하려는 힘과, 반대로 범용적으로 설계된 연산장치를 작업의 특성에 맞게 분화시키려는 힘이 교차하며 발전해 왔는데요, 이번 주의 learning snack에서는 이 두 트렌드가 어떻게 상호작용하며 오늘날의 AI 가속기들로 발전해 왔는지 &amp; 그 미래는 어떨지 짚어보도록 하겠습니다.\n \n오늘날 AI 작업에 쓰이는 가장 흔한 형태의 가속기는 GPU입니다. 그렇지만 처음부터 GPU를 AI 작업 - 구체적으로, \"연산\"에 활용하던 것은 아니었습니다. 프로그래머블하고 가장 범용적인 형태의 연산을 담당하는 장치의 위상은 꽤 오랜 기간 CPU가 독점하고 있었거든요. 사실 GPU라는 이름 자체가 대중화된 것도 컴퓨팅의 역사에서는 그리 오래되지 않았습니다. 3D 그래픽 화면을 그려내기 위해서 수행해야 하던 작업 중 물체의 변형과 광원 두 가지 작업을 처리할 수 있는 전용 하드웨어를 출시하며 CPU의 워크로드를 분담하게 되었고, 이러한 기하학(geometry)적 연산을 CPU로부터 이양하게 된 이 최초의 하드웨어를 개발사는 \"그래픽처리장치graphics processing unit(GPU)\"로 정의했습니다. 1999년 GeForce(\"Geometric Force\")의 등장입니다.\n \n이후 GeForce의 텍스처 파이프라인을 두 배로 늘려 초당 10억(giga)개의 텍스처 정보(texel)를 처리할 수 있게 한 GeForce 2 GTS (\"Giga TexelS\"), 프로그래머블한 쉐이더 파이프라인을 도입한 최초의 GPU인 GeForce 3, PCI-Express 인터페이스를 적용한 GeForce 4, 내부 부동소수점 벡터 포맷을 24비트에서 32비트로 확장한 GeForce FX를 거쳐 마침내 2006년 GeForce 8xxx 세대에서 '통합 쉐이더unified shader' 라는 기념비적 전환이 발표됩니다. 여기서 잠시, GPU 내부의 그래픽 처리 파이프라인을 간단히 짚고 넘어가겠습니다.\n \n \n2.\n \n3D 그래픽을 표현하기 위해 기하학적인 연산이 요구된다는 점을 앞에서 언급했는데, 구체적으로 GeForce 8xxx 이전까지의 GPU 내부에서는 물체의 꼭지점 정보를 처리하는 장치(\"vertex shader\")와 픽셀 정보를 처리하는 장치(\"pixel shader\")가 함께 존재했고 이에 따라 물체의 3D 메쉬 정보를 뒷 단계에 보내기까지의 로드 설계에 많은 노력이 들어갔습니다. (꼭지점을 다 그려도 픽셀 정보가 완료되기 전에는 다음 단계로 넘어갈 수 없고 그 반대도 마찬가지입니다) GeForce 8xxx 부터는 이것을 '통합 쉐이더unified shader'라는 범용적인 ALU로 치환하여, CPU 내의 FP 파이프라인과 유사한 연산장치를 다수 탑재하는 방향으로 전환한 것입니다. 이로써 후대의 GPU는 3D 메쉬 정보를 일원화된(\"unified\") 컴퓨팅 자원에 맡기게 되어 효율을 크게 높였고, 한편으로는 그래픽 작업이 아닌 다른 범용 연산에도 대응할 수 있게 되어 용도가 크게 확장되었습니다. NVIDIA는 이 unified shader를 \"Compute Unified Device Architecture(CUDA) core\"라 이름붙이고, CUDA core를 사용할 수 있는 라이브러리를 개발해 공개하기 시작했습니다. 오늘날 CUDA 생태계의 시작입니다.\n \n그럼에도 GPU에는 아직 여러 다양한 '특화 유닛'이 존재합니다. Unified shader가 3D 구조를 완성하더라도 이렇게 생성된 다각형의 각 면에 표면(texture) 정보를 입혀야(mapping) 하며, 표면 정보까지 입혀져 색칠된 3D 공간 정보가 완성되면 다시 이것을 매초, 매 프레임마다 우리가 모니터를 통해 보게 되는 2차원 평면화(rasterization)를 거쳐 납작하게 투영한 이미지로 그려내야 합니다. 이 나열된 과정들은 각각 텍스처 매핑 유닛(texture mapping unit)과 래스터라이저(rasterizer)라는 전용 유닛들이 담당하게 되며, 따라서 오늘날 GPU가 화면을 그려내는 과정을 아주 간단한 흐름도로 나타내면 아래와 같습니다.\n \n그래픽 메모리에서 정보 읽어오기 -&gt; 1. Unified shader의 연산(3D 구조 생성) -&gt; 2. 텍스처 매핑 유닛의 연산(표면 입히기) -&gt; 3. 래스터라이저의 연산(2D 평면화, 매 프레임 생성) \n여기에서 2번과 3번을 제외하면, unified shader를 활용해 GPU를 '연산' 용도만으로 활용하는 오늘날의 General-Purpose(범용) GPU(\"GPGPU\") 컴퓨팅의 흐름도가 됩니다. GPU의 성능을 GPU가 어떤 작업을 처리하는 데 소요되는 총 시간의 역수로 정의한다면, 다시 위의 각 과정에 소요되는 시간들의 합의 역수로 구할 수 있습니다. 그리고 우리가 학생 시절 배운 산술\/기하\/조화평균의 정의에 따라, 각 연산유닛의 성능 사이의 조화평균값(=역수의 평균)은 GPU 연산성능의 척도가 됩니다. 재미있는 점은 여기서 가장 전통적인 '그래픽' 성능에 기여율이 높은 항은 래스터라이저의 성능이고, '연산' 성능에 기여율이 높은 항은 역시 unified shader의 성능이 된다는 점입니다. 그럼 가장 기여율이 적은 항은 어디일까요? 바로 0번, 그래픽 메모리의 성능(대역폭)입니다.\n \n \n3.\n \n한편 GPGPU 컴퓨팅의 생태계가 확장되며 GPU가 처리하는 데이터의 포맷을 다양화하려는 목소리가 커졌습니다. 2000년대 초반까지만 하더라도 '컴퓨팅', 곧 슈퍼컴퓨터의 덕목은 '인간이 할 수 없는 고도로 정밀한 연산'을 수행하여 인간이 진출할 지평을 넓히는 데 있었습니다. 1차대전 전후 포탄의 탄도를 계산하던 연구는 오늘의 로켓 과학의 근간이 되었고 인류가 보통의 정밀도에서는 상상할 수도 없었을 천체물리학을 통한 스윙바이와 전인미답의 원거리 탐사를 가능하게 했습니다. 이를 위해 그동안의 부동소수점 연산 포맷의 표준이던 FP32(\"단정밀도single precision\"라는 이름부터 FP32가 당대의 컴퓨팅의 기준점임을 암시합니다)를 넘어, 두 배 넓은 포맷인 FP64(\"배정밀도double precision\")에 대응하는 연산 유닛이 탑재됩니다. NVIDIA의 Tesla 라는 연산 전용 GPU가 여기에 아주 좋은 성능을 제공했고, 그 흔적은 Tesla의 컨슈머 버전인 GeForce TITAN에 남아 있습니다. (TITAN을 사용해본 경험이 있으시다면, NVIDIA 제어판에서 \"Double Precision 활성화\" 옵션을 켜 보신 적이 있을 것 같습니다)\n \n그러나 2010년대 중반 이후 Convolutional 신경망(CNN), 딥 러닝이 GPGPU 컴퓨팅의 거대한 수요처가 되며 패러다임이 변화하기 시작합니다. 하나는 unified shader로부터 행렬연산에 최적화된 전용 연산유닛의 재분화가 그것이고, 다른 하나는 정밀도를 오히려 떨어뜨리려는 움직임입니다. 이후 행렬연산 전용 유닛인 \"텐서 코어tensor core\"가 도입되며 모든 범용연산을 unified shader가 담당하던 체제는 다시 막을 내렸고, FP32보다 간결한 포맷인 FP16(\"반정밀도half precision\"), 다시 그 절반인 INT8(8비트 정수), 그 절반인 INT4(4비트 정수) 등이 도입되었지요. 가장 큰 이유는, 인간의 뉴런을 모방하는 데 있어서 \"그렇게까지 정밀한 연산을 할 필요가 없다\" 다시 말해 \"낮은 정밀도의 연산을 더 많이 수행해 신경망의 중첩적인 수행을 모방하는 것\" 이 인간 뇌의 동작원리와 더 유사하고, 실제로 더 비슷한 결과값을 얻기 시작했기 때문입니다. 아주 거칠게 대입하면 FP64 연산 하나를 수행할 수 있는 규모의 칩 설계 자원(=트랜지스터 수)으로 FP32 두 개, FP16 네 개, INT8 여덟 개, INT4 열여섯 개를 수행할 수 있다면 개별 연산의 정밀도를 희생해서라도 2~16배 많은 연산을 수행해 CNN의 각 레이어를 더 많이 학습시키는 것이 훨씬 '인간적인' 결과를 만들 수 있었다는 이야기입니다.\n \n이렇게 정밀도를 반감시키며 총연산량(throughput)을 배증시키는 것은 오늘날 빅테크 다수가 채택한 전략입니다. NVIDIA의 B100 GPU가 H100보다 성능이 두 배 올랐다거나, Apple의 M4 SoC가 M3대비 NPU 성능이 두 배 오른 것은 모두 칩 설계 자원의 증가보다는 INT8 -&gt; INT4로의 이행에 의존하고 있습니다. 나아가 FP32 포맷을 구성하는 32개의 비트값에서, 유효숫자를 표현하는 23개의 비트와 지수를 표현하는 8개의 비트를 비슷한 비율로 줄인  FP16(유효숫자10+지수5)은 그 포맷이 간결해진 만큼 표현할 수 있는 수의 범위에도 차이가 있었는데(FP32는 10의 -8승부터 8승까지, FP16은 10의 -5승부터 5승까지), 아예 유효숫자를 대폭 줄이고 표현범위 자체는 그대로 유지하여(유효숫자7+지수8) 딥러닝에 특화된 새로운 포맷인 \"Brain\" BF16이 제안되기도 했습니다. 컴퓨팅의 패러다임이 \"인간이 할 수 없는 고도로 정밀한 연산을 하는 것\" 에서 \"인간의 뇌가 하듯 별로 정밀하지 않은 연산을, 대신 아주 많이 하는 것\" 으로 거대하게 방향을 틀어가고 있는 것입니다.\n \n심지어 지금도, INT4보다도 더 간소한 포맷으로의 탐구는 멈추지 않고 있습니다. 최근에는 1.58비트(2^1.58=3, -1, 0, 1의 3개 값을 표현하는 2의 지수값이 log_2(3)=1.5849이므로) 양자화, Boolean=1비트 (0, 1) 포맷 등 극도로 추상화된 포맷을 통해 모델을 압축하는 기법도 진지하게 논의되고 있는 시점입니다.\n \n \n4.\n \n왜 데이터 포맷을 끊임없이 간소화하는지 짚어 보자면 결국 파라미터 수가 기하급수적으로 늘어나는 오늘날의 LLM을 구동하기 위한 (특히 추론 작업에서) 그래픽 메모리 공간과 타협하기 위해서입니다. 앞서 GPU의 성능에 가장 기여도가 적은 요소가 그래픽 메모리의 대역폭이라고 말씀드렸기에, 스크롤의 압박을 이겨내고 이 글을 쭉 읽어내려오셨다면 위 두 명제가 모순이 아니냐고 생각하실 분이 계실 것 같네요. 여기는 한 가지 함정이 있습니다. 바로 성능에 기여하는 정도(통계학의 term으로 \"설명력\"이라고 합니다)는 다른 요소에 비해 메모리 대역폭이 가장 낮았던 것이 맞지만, GPU의 성능과 각 요소간의 상관관계correlation를 구해 보면 반대로 메모리 대역폭과의 상관관계가 가장 높게 나타나기 때문입니다. 심지어 제조사가 광고하는 GPU의 peak computing power(=unified shader 개수와 동작 클럭의 곱)보다도 correlation이 높습니다. 즉 메모리 대역폭은 그 세대의 GPU 성능수준을 원천적으로 결정짓는 변수이고, 여기서 도출된 GPU의 설계상 잠재력을 넘어 대역폭이 높아진다고 성능향상이 추가로 주어지지는 않지만 반대로 대역폭이 줄어들면 큰 성능하락을 보게 되는, 가장 근본적인 limiting factor라는 점입니다.\n \n또한 성능, 전력, 열이 고도로 집약된 데이터센터에서 \"성능\" 은 더 이상 독립된 변수로 존재할 수 없게 되었습니다. 데이터센터가 입주한 건물, 지역, 국가가 공급할 수 있는 전력의 총량과 열적 균형을 유지할 수 있는 열용량의 총량에 종속된 함수로서 \"성능\"이 도출되게 된 것이죠. 이러한 열설계전력에 가장 큰 병목으로 작용하는 요소는 unified shader가 소비하는 전력도, tensor core가 소비하는 전력도 아닌 메모리에서 1비트를 처리하는 데 필요한 전력입니다. 이를 타개하기 위해 데이터 포맷 간소화를 통한 경량화, 모델 자체의 파라미터 소거를 통한 압축(pruning이라는 기법을 이용하는데 글이 이미 산으로 가고 있기 때문에 여기서는 더 다루지 않겠습니다), 그리고 하드웨어 레벨에서는 메모리 계층 구조(memory hierarchy) 자체를 개혁하는 방법 등이 있습니다. 대표적인 세번째 예시는 캐시메모리의 용량을 칩 전체 규모 대비 크게 늘린 Apple의 SoC들, 그리고 극단적으로 모든 메모리계층을 SRAM으로 단일화한 GROQ 등의 사례가 있으나 칩의 비용최적화 측면에서 대중적인 해결책으로 보기 어렵다는 것이 아직까지의 연구자들의 시각입니다. 결국은 주어진 조건 아래 메모리를 최대한 알뜰하게 사용하는 것이 AI의 성능을 높이는 길인 것이지요.\n \n주로 제조사의 접근법에서는 일반론적인 표준, 데이터 포맷 자체를 재정의하며 모든 모델\/서비스에 범용적으로 적용될 수 있는 방향으로 경량화 = 메모리(VRAM) 효율 = GPU 효율 = 궁극적으로 데이터센터의 전력효율을 유도하고 있으나, LLM을 개발하고 서비스를 운용하는 회사들은 조금 시각이 다릅니다. NVIDIA는 Blackwell을 발표하며 pruning 기법 중 하나로 인접한 파라미터의 중복값을 기계적으로 소거하는 sparsity 알고리즘을 공개했으나 (제조사의 view), 이렇게 압축하면 메모리 용량을 덜 먹는데도 성능이 오히려 떨어집니다 (서비스사의 view). 서비스사가 연구하는 인공신경망의 성능을 높일 수 있는 방법론은 아직까지 정형성이 도출되지 않아 하드웨어로 구현하기가 어렵고, 제조사가 제안하는 정형적인 방법론은 인공신경망의 성능향상으로 이어지지 않는 교착상태에 있는 것이죠. 물론 여기서 돌파구를 찾기 위한 서비스사-제조사간의 합종연횡 역시 이어지고 있습니다. Microsoft의 자체 가속기 Maia도 이러한 흐름에 한 발을 내딛는 것이길, 향후에도 더 흥미로운 자체 AI 인프라스트럭처를 구축해 고객들에게 소개할 수 있기를 바라며, 이번 learning snack을 맺어 봅니다. (*회사 연재분은 여기서 끝)\n\n-\n\n닥터몰라를 지켜봐 오신 분들이라면 금방 떠올리실 수 있는 소재가 사용되었는데, 바로 GPU 성능 시뮬레이터가 그것입니다. GPU 시뮬레이터는 주어진 여러 독립변수를 조합해 다시 외부에서 주어진 성능이라는 종속변수를 이끌어내는 모델이었는데 항의 개수가 한정된 다항식의 특성상 오버피팅을 피할 수 없었습니다. 메모리 대역폭의 성능에의 기여도와 상관관계의 역설은 당시 실험을 통해 얻은 재미있는 트리비아 중 하나였습니다.\n\n시뮬레이션 수식 내의 가중치가 가장 적음에도, 종속변수인 성능과의 상관관계가 가장 크다는 역설.\n\n처음에는 칩메이커들이 내부적으로 시뮬레이션을 돌려 그에 가장 알맞은 메모리 스펙을 '고르는' 것이 아닐까 생각했으나, 딥러닝의 시대에 접어들며 메모리의 한계에 괴로워하는(?) 칩메이커들을 보니 그리 여유롭게 스펙을 '고를' 형편이 아니었구나라는 쪽으로 생각을 고치게 되었고. 그보다는 메모리 대역폭이 GPU의 성능 수준을 근본적으로 결정하는 hard limit이라고 여기게 되었습니다.\n\n한편 위의 글에 다 담지 못한 트리비아 중에는 각 컴퓨팅 리소스간의 밸런스에 대한 저희의 소감이 있는데요, GPU 내 리소스의 밸런스를 가장 잘 잡는다는 측면에서 NVIDIA는 단연 독보적으로 보였습니다. 반면 AMD는 리소스간 밸런스 설계 측면에서 이해할 수 없는 결과를 내놓는 경우가 많았습니다. 메모리 대역폭에 비해 래스터라이저가 너무 부족했던 Fiji, 그리고 그 이후에는 래스터라이저와 메모리 대역폭의 불균형은 해소되었으나 범용 연산유닛의 비중이 항상 과하게 유지되었던 점 등을 꼽을 수 있겠네요. 다만 시간이 많이 지난 지금, 핵심은 그게 아니었다는 생각이 듭니다. 각 칩메이커의 설계는 각자가 이상적으로 생각하는 미래를 선반영하는 것이라 본다면 AMD는 범용 연산유닛이 항상 당대보다 더 많이 쓰일 미래를 염두에 둔 것이었겠지요.\n\n끝으로 딥러닝 시대의 초입이었던 2017년, 한 신문에 연재했던 글을 오랜만에 다시 생각해보며 글을 맺어봅니다.\n\n-\n\n\"인간이 다른 척색동물과 구분되는 가장 큰 차이는 ‘파충류의 뇌’ 바깥을 둘러싼 컴퓨팅 파워의 집합체 회백질이 고도로 발달해 있다는 점이다. 불행하게도 생물의 진화속도보다 컴퓨터의 발전속도가 월등히 빨랐던 탓에 오늘날 회백질은, 인류 문명이라는 그간 빛나는 성취에도 불구하고 지구상에 존재하는 가장 뛰어난 컴퓨터가 아니게 됐다. (중략)\n\n하지만 그것마저 본질은 아니다. 사실 오늘날 가장 발전된 로봇 기술은 방정식을 사람보다 빨리 풀거나 암호를 사람보다 빨리 맞추는 것에 있지 않다. 가장 뛰어난 과학자들이 모여 구현하는 것이라곤 어이없게도 갓 말을 뗀 어린이 수준의 인지능력이나 걸음마를 막 시작한 아이 정도의 보행능력이니. 인공지능의 미래가 ‘지능’ 그 자체는 아니라는 얘기다.\n\n인간에게 가장 쉬운 것이 컴퓨터에게는 가장 어렵고, 실은 인간 스스로도 ‘파충류의 뇌’의 관할인 반사신경과 불수의근의 도움이 없다면 도저히 불가능한 작업인 그것. 요컨대 인공지능의 미래가 ‘본능’에 있다는 하드웨어 제조사의 감각적이기 그지없는 작명은 결코 틀리지 않았을 뿐 아니라 실은 너무나 사실에 가까운 것이다. 회백질의 영역을 이미 넘어선 컴퓨터는 그 안으로 파고들어 인간의 본능을 모방하려 한다. (중략)\n\n오늘날 컴퓨터그래픽은 컴퓨터의 연산성능을 가장 많이 요구하는 분야 중 하나다. 공교롭게도 이 사상 역시 고성능의 단일 CPU에 의존적이던 당시의 기술을, 비록 개별 CPU의 성능은 낮더라도 이들을 여럿 묶어 성능의 총합을 높일 수 있다면 그에 비례해 그래픽 수준을 높일 수 있도록 하는 것이 핵심이었다. 이 프로젝트의 이름은 ‘맨틀(Mantle)’로 오늘날 컴퓨터 그래픽 표준인 다이렉트X 12(DirectX 12), 벌칸(Vulkan) 등에 계승됐다.\n\n한때 하드웨어 제조사들은 컴퓨터그래픽의 미래를 지구 깊은 곳 맨틀에서 찾으려 했다. 지금 그들은 우리 두뇌의 더 깊은 곳에서 컴퓨팅의 미래를 찾으려 한다. 공교롭게도 지구와 두뇌는 많은 면에서 닮았다. 산업혁명 이래 폭발적으로 지적 탐구가 행해졌음에도 아직까지도 지각 아래쪽은 인간의 눈으로 탐사된 바 없고, 여전히 회백질 아래 ‘파충류의 뇌’의 작동원리는 완벽히 드러나지 않았다.\n\n깊어질수록 닿기 어려운 지구, 파고들수록 지능과는 멀어지고 본능에 가까워지는 뇌. 이들은 불연속면으로 분절된 물리적 구조만큼이나 인류 지식의 지평이 닿아 있는 정도까지도 닮아 있다. 어쩌면 가장 익숙하고 가까운 대상의 가장 낯선 부분이기에 단연 인류 최후의 탐험지가 될 수밖에 없는 그곳. 거기에 컴퓨터의 미래가 있다.\"\n\n(<https:\/\/www.edaily.co.kr\/news\/read?newsId=01515366615990256|https:\/\/www.edaily.co.kr\/news\/read?newsId=01515366615990256> )\n\n",
        "team": "T05UGFFGL07",
        "user_team": "T05UGFFGL07",
        "source_team": "T05UGFFGL07",
        "user_profile": {
            "avatar_hash": "34f110a5edc4",
            "image_72": "https:\/\/avatars.slack-edge.com\/2023-11-14\/6197301711475_34f110a5edc435398e63_72.png",
            "first_name": "김용담",
            "real_name": "김용담",
            "display_name": "김용담 강사",
            "team": "T05UGFFGL07",
            "name": "codingiscoffee",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "LNzYG",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "\n"
                            },
                            {
                                "type": "text",
                                "text": "\nLLM에서 왜 quantization이 중요해졌는지에 대해 GPU의 발전역사와 연관되어 자세하게 설명된 글이 있어 공유해봅니다!\n\n\n\n(5\/27 Microsoft 사내 채널 릴레이 연재글입니다)\n\nAI 가속기의 과거, 현재, 미래\n\n1.\n \nMicrosoft에서 제공하는 많은 AI 서비스의 중심에는 VM으로 대표되는 Azure AI 인프라스트럭처가 있습니다. 특히 일반적인 범용 컴퓨팅용 VM에 더해 AI 작업에 요구되는 특화된 연산성능을 강화한 가속기를 탑재한 포트폴리오도 점점 다양해지고 있지요. 역사적으로(?) 컴퓨팅 성능을 향상시키기 위한 트렌드는 파편화된 연산장치를 하나로 통합하려는 힘과, 반대로 범용적으로 설계된 연산장치를 작업의 특성에 맞게 분화시키려는 힘이 교차하며 발전해 왔는데요, 이번 주의 learning snack에서는 이 두 트렌드가 어떻게 상호작용하며 오늘날의 AI 가속기들로 발전해 왔는지 & 그 미래는 어떨지 짚어보도록 하겠습니다.\n \n오늘날 AI 작업에 쓰이는 가장 흔한 형태의 가속기는 GPU입니다. 그렇지만 처음부터 GPU를 AI 작업 - 구체적으로, \"연산\"에 활용하던 것은 아니었습니다. 프로그래머블하고 가장 범용적인 형태의 연산을 담당하는 장치의 위상은 꽤 오랜 기간 CPU가 독점하고 있었거든요. 사실 GPU라는 이름 자체가 대중화된 것도 컴퓨팅의 역사에서는 그리 오래되지 않았습니다. 3D 그래픽 화면을 그려내기 위해서 수행해야 하던 작업 중 물체의 변형과 광원 두 가지 작업을 처리할 수 있는 전용 하드웨어를 출시하며 CPU의 워크로드를 분담하게 되었고, 이러한 기하학(geometry)적 연산을 CPU로부터 이양하게 된 이 최초의 하드웨어를 개발사는 \"그래픽처리장치graphics processing unit(GPU)\"로 정의했습니다. 1999년 GeForce(\"Geometric Force\")의 등장입니다.\n \n이후 GeForce의 텍스처 파이프라인을 두 배로 늘려 초당 10억(giga)개의 텍스처 정보(texel)를 처리할 수 있게 한 GeForce 2 GTS (\"Giga TexelS\"), 프로그래머블한 쉐이더 파이프라인을 도입한 최초의 GPU인 GeForce 3, PCI-Express 인터페이스를 적용한 GeForce 4, 내부 부동소수점 벡터 포맷을 24비트에서 32비트로 확장한 GeForce FX를 거쳐 마침내 2006년 GeForce 8xxx 세대에서 '통합 쉐이더unified shader' 라는 기념비적 전환이 발표됩니다. 여기서 잠시, GPU 내부의 그래픽 처리 파이프라인을 간단히 짚고 넘어가겠습니다.\n \n \n2.\n \n3D 그래픽을 표현하기 위해 기하학적인 연산이 요구된다는 점을 앞에서 언급했는데, 구체적으로 GeForce 8xxx 이전까지의 GPU 내부에서는 물체의 꼭지점 정보를 처리하는 장치(\"vertex shader\")와 픽셀 정보를 처리하는 장치(\"pixel shader\")가 함께 존재했고 이에 따라 물체의 3D 메쉬 정보를 뒷 단계에 보내기까지의 로드 설계에 많은 노력이 들어갔습니다. (꼭지점을 다 그려도 픽셀 정보가 완료되기 전에는 다음 단계로 넘어갈 수 없고 그 반대도 마찬가지입니다) GeForce 8xxx 부터는 이것을 '통합 쉐이더unified shader'라는 범용적인 ALU로 치환하여, CPU 내의 FP 파이프라인과 유사한 연산장치를 다수 탑재하는 방향으로 전환한 것입니다. 이로써 후대의 GPU는 3D 메쉬 정보를 일원화된(\"unified\") 컴퓨팅 자원에 맡기게 되어 효율을 크게 높였고, 한편으로는 그래픽 작업이 아닌 다른 범용 연산에도 대응할 수 있게 되어 용도가 크게 확장되었습니다. NVIDIA는 이 unified shader를 \"Compute Unified Device Architecture(CUDA) core\"라 이름붙이고, CUDA core를 사용할 수 있는 라이브러리를 개발해 공개하기 시작했습니다. 오늘날 CUDA 생태계의 시작입니다.\n \n그럼에도 GPU에는 아직 여러 다양한 '특화 유닛'이 존재합니다. Unified shader가 3D 구조를 완성하더라도 이렇게 생성된 다각형의 각 면에 표면(texture) 정보를 입혀야(mapping) 하며, 표면 정보까지 입혀져 색칠된 3D 공간 정보가 완성되면 다시 이것을 매초, 매 프레임마다 우리가 모니터를 통해 보게 되는 2차원 평면화(rasterization)를 거쳐 납작하게 투영한 이미지로 그려내야 합니다. 이 나열된 과정들은 각각 텍스처 매핑 유닛(texture mapping unit)과 래스터라이저(rasterizer)라는 전용 유닛들이 담당하게 되며, 따라서 오늘날 GPU가 화면을 그려내는 과정을 아주 간단한 흐름도로 나타내면 아래와 같습니다.\n \n그래픽 메모리에서 정보 읽어오기 -> 1. Unified shader의 연산(3D 구조 생성) -> 2. 텍스처 매핑 유닛의 연산(표면 입히기) -> 3. 래스터라이저의 연산(2D 평면화, 매 프레임 생성) \n여기에서 2번과 3번을 제외하면, unified shader를 활용해 GPU를 '연산' 용도만으로 활용하는 오늘날의 General-Purpose(범용) GPU(\"GPGPU\") 컴퓨팅의 흐름도가 됩니다. GPU의 성능을 GPU가 어떤 작업을 처리하는 데 소요되는 총 시간의 역수로 정의한다면, 다시 위의 각 과정에 소요되는 시간들의 합의 역수로 구할 수 있습니다. 그리고 우리가 학생 시절 배운 산술\/기하\/조화평균의 정의에 따라, 각 연산유닛의 성능 사이의 조화평균값(=역수의 평균)은 GPU 연산성능의 척도가 됩니다. 재미있는 점은 여기서 가장 전통적인 '그래픽' 성능에 기여율이 높은 항은 래스터라이저의 성능이고, '연산' 성능에 기여율이 높은 항은 역시 unified shader의 성능이 된다는 점입니다. 그럼 가장 기여율이 적은 항은 어디일까요? 바로 0번, 그래픽 메모리의 성능(대역폭)입니다.\n \n \n3.\n \n한편 GPGPU 컴퓨팅의 생태계가 확장되며 GPU가 처리하는 데이터의 포맷을 다양화하려는 목소리가 커졌습니다. 2000년대 초반까지만 하더라도 '컴퓨팅', 곧 슈퍼컴퓨터의 덕목은 '인간이 할 수 없는 고도로 정밀한 연산'을 수행하여 인간이 진출할 지평을 넓히는 데 있었습니다. 1차대전 전후 포탄의 탄도를 계산하던 연구는 오늘의 로켓 과학의 근간이 되었고 인류가 보통의 정밀도에서는 상상할 수도 없었을 천체물리학을 통한 스윙바이와 전인미답의 원거리 탐사를 가능하게 했습니다. 이를 위해 그동안의 부동소수점 연산 포맷의 표준이던 FP32(\"단정밀도single precision\"라는 이름부터 FP32가 당대의 컴퓨팅의 기준점임을 암시합니다)를 넘어, 두 배 넓은 포맷인 FP64(\"배정밀도double precision\")에 대응하는 연산 유닛이 탑재됩니다. NVIDIA의 Tesla 라는 연산 전용 GPU가 여기에 아주 좋은 성능을 제공했고, 그 흔적은 Tesla의 컨슈머 버전인 GeForce TITAN에 남아 있습니다. (TITAN을 사용해본 경험이 있으시다면, NVIDIA 제어판에서 \"Double Precision 활성화\" 옵션을 켜 보신 적이 있을 것 같습니다)\n \n그러나 2010년대 중반 이후 Convolutional 신경망(CNN), 딥 러닝이 GPGPU 컴퓨팅의 거대한 수요처가 되며 패러다임이 변화하기 시작합니다. 하나는 unified shader로부터 행렬연산에 최적화된 전용 연산유닛의 재분화가 그것이고, 다른 하나는 정밀도를 오히려 떨어뜨리려는 움직임입니다. 이후 행렬연산 전용 유닛인 \"텐서 코어tensor core\"가 도입되며 모든 범용연산을 unified shader가 담당하던 체제는 다시 막을 내렸고, FP32보다 간결한 포맷인 FP16(\"반정밀도half precision\"), 다시 그 절반인 INT8(8비트 정수), 그 절반인 INT4(4비트 정수) 등이 도입되었지요. 가장 큰 이유는, 인간의 뉴런을 모방하는 데 있어서 \"그렇게까지 정밀한 연산을 할 필요가 없다\" 다시 말해 \"낮은 정밀도의 연산을 더 많이 수행해 신경망의 중첩적인 수행을 모방하는 것\" 이 인간 뇌의 동작원리와 더 유사하고, 실제로 더 비슷한 결과값을 얻기 시작했기 때문입니다. 아주 거칠게 대입하면 FP64 연산 하나를 수행할 수 있는 규모의 칩 설계 자원(=트랜지스터 수)으로 FP32 두 개, FP16 네 개, INT8 여덟 개, INT4 열여섯 개를 수행할 수 있다면 개별 연산의 정밀도를 희생해서라도 2~16배 많은 연산을 수행해 CNN의 각 레이어를 더 많이 학습시키는 것이 훨씬 '인간적인' 결과를 만들 수 있었다는 이야기입니다.\n \n이렇게 정밀도를 반감시키며 총연산량(throughput)을 배증시키는 것은 오늘날 빅테크 다수가 채택한 전략입니다. NVIDIA의 B100 GPU가 H100보다 성능이 두 배 올랐다거나, Apple의 M4 SoC가 M3대비 NPU 성능이 두 배 오른 것은 모두 칩 설계 자원의 증가보다는 INT8 -> INT4로의 이행에 의존하고 있습니다. 나아가 FP32 포맷을 구성하는 32개의 비트값에서, 유효숫자를 표현하는 23개의 비트와 지수를 표현하는 8개의 비트를 비슷한 비율로 줄인  FP16(유효숫자10+지수5)은 그 포맷이 간결해진 만큼 표현할 수 있는 수의 범위에도 차이가 있었는데(FP32는 10의 -8승부터 8승까지, FP16은 10의 -5승부터 5승까지), 아예 유효숫자를 대폭 줄이고 표현범위 자체는 그대로 유지하여(유효숫자7+지수8) 딥러닝에 특화된 새로운 포맷인 \"Brain\" BF16이 제안되기도 했습니다. 컴퓨팅의 패러다임이 \"인간이 할 수 없는 고도로 정밀한 연산을 하는 것\" 에서 \"인간의 뇌가 하듯 별로 정밀하지 않은 연산을, 대신 아주 많이 하는 것\" 으로 거대하게 방향을 틀어가고 있는 것입니다.\n \n심지어 지금도, INT4보다도 더 간소한 포맷으로의 탐구는 멈추지 않고 있습니다. 최근에는 1.58비트(2^1.58=3, -1, 0, 1의 3개 값을 표현하는 2의 지수값이 log_2(3)=1.5849이므로) 양자화, Boolean=1비트 (0, 1) 포맷 등 극도로 추상화된 포맷을 통해 모델을 압축하는 기법도 진지하게 논의되고 있는 시점입니다.\n \n \n4.\n \n왜 데이터 포맷을 끊임없이 간소화하는지 짚어 보자면 결국 파라미터 수가 기하급수적으로 늘어나는 오늘날의 LLM을 구동하기 위한 (특히 추론 작업에서) 그래픽 메모리 공간과 타협하기 위해서입니다. 앞서 GPU의 성능에 가장 기여도가 적은 요소가 그래픽 메모리의 대역폭이라고 말씀드렸기에, 스크롤의 압박을 이겨내고 이 글을 쭉 읽어내려오셨다면 위 두 명제가 모순이 아니냐고 생각하실 분이 계실 것 같네요. 여기는 한 가지 함정이 있습니다. 바로 성능에 기여하는 정도(통계학의 term으로 \"설명력\"이라고 합니다)는 다른 요소에 비해 메모리 대역폭이 가장 낮았던 것이 맞지만, GPU의 성능과 각 요소간의 상관관계correlation를 구해 보면 반대로 메모리 대역폭과의 상관관계가 가장 높게 나타나기 때문입니다. 심지어 제조사가 광고하는 GPU의 peak computing power(=unified shader 개수와 동작 클럭의 곱)보다도 correlation이 높습니다. 즉 메모리 대역폭은 그 세대의 GPU 성능수준을 원천적으로 결정짓는 변수이고, 여기서 도출된 GPU의 설계상 잠재력을 넘어 대역폭이 높아진다고 성능향상이 추가로 주어지지는 않지만 반대로 대역폭이 줄어들면 큰 성능하락을 보게 되는, 가장 근본적인 limiting factor라는 점입니다.\n \n또한 성능, 전력, 열이 고도로 집약된 데이터센터에서 \"성능\" 은 더 이상 독립된 변수로 존재할 수 없게 되었습니다. 데이터센터가 입주한 건물, 지역, 국가가 공급할 수 있는 전력의 총량과 열적 균형을 유지할 수 있는 열용량의 총량에 종속된 함수로서 \"성능\"이 도출되게 된 것이죠. 이러한 열설계전력에 가장 큰 병목으로 작용하는 요소는 unified shader가 소비하는 전력도, tensor core가 소비하는 전력도 아닌 메모리에서 1비트를 처리하는 데 필요한 전력입니다. 이를 타개하기 위해 데이터 포맷 간소화를 통한 경량화, 모델 자체의 파라미터 소거를 통한 압축(pruning이라는 기법을 이용하는데 글이 이미 산으로 가고 있기 때문에 여기서는 더 다루지 않겠습니다), 그리고 하드웨어 레벨에서는 메모리 계층 구조(memory hierarchy) 자체를 개혁하는 방법 등이 있습니다. 대표적인 세번째 예시는 캐시메모리의 용량을 칩 전체 규모 대비 크게 늘린 Apple의 SoC들, 그리고 극단적으로 모든 메모리계층을 SRAM으로 단일화한 GROQ 등의 사례가 있으나 칩의 비용최적화 측면에서 대중적인 해결책으로 보기 어렵다는 것이 아직까지의 연구자들의 시각입니다. 결국은 주어진 조건 아래 메모리를 최대한 알뜰하게 사용하는 것이 AI의 성능을 높이는 길인 것이지요.\n \n주로 제조사의 접근법에서는 일반론적인 표준, 데이터 포맷 자체를 재정의하며 모든 모델\/서비스에 범용적으로 적용될 수 있는 방향으로 경량화 = 메모리(VRAM) 효율 = GPU 효율 = 궁극적으로 데이터센터의 전력효율을 유도하고 있으나, LLM을 개발하고 서비스를 운용하는 회사들은 조금 시각이 다릅니다. NVIDIA는 Blackwell을 발표하며 pruning 기법 중 하나로 인접한 파라미터의 중복값을 기계적으로 소거하는 sparsity 알고리즘을 공개했으나 (제조사의 view), 이렇게 압축하면 메모리 용량을 덜 먹는데도 성능이 오히려 떨어집니다 (서비스사의 view). 서비스사가 연구하는 인공신경망의 성능을 높일 수 있는 방법론은 아직까지 정형성이 도출되지 않아 하드웨어로 구현하기가 어렵고, 제조사가 제안하는 정형적인 방법론은 인공신경망의 성능향상으로 이어지지 않는 교착상태에 있는 것이죠. 물론 여기서 돌파구를 찾기 위한 서비스사-제조사간의 합종연횡 역시 이어지고 있습니다. Microsoft의 자체 가속기 Maia도 이러한 흐름에 한 발을 내딛는 것이길, 향후에도 더 흥미로운 자체 AI 인프라스트럭처를 구축해 고객들에게 소개할 수 있기를 바라며, 이번 learning snack을 맺어 봅니다. (*회사 연재분은 여기서 끝)\n\n-\n\n닥터몰라를 지켜봐 오신 분들이라면 금방 떠올리실 수 있는 소재가 사용되었는데, 바로 GPU 성능 시뮬레이터가 그것입니다. GPU 시뮬레이터는 주어진 여러 독립변수를 조합해 다시 외부에서 주어진 성능이라는 종속변수를 이끌어내는 모델이었는데 항의 개수가 한정된 다항식의 특성상 오버피팅을 피할 수 없었습니다. 메모리 대역폭의 성능에의 기여도와 상관관계의 역설은 당시 실험을 통해 얻은 재미있는 트리비아 중 하나였습니다.\n\n시뮬레이션 수식 내의 가중치가 가장 적음에도, 종속변수인 성능과의 상관관계가 가장 크다는 역설.\n\n처음에는 칩메이커들이 내부적으로 시뮬레이션을 돌려 그에 가장 알맞은 메모리 스펙을 '고르는' 것이 아닐까 생각했으나, 딥러닝의 시대에 접어들며 메모리의 한계에 괴로워하는(?) 칩메이커들을 보니 그리 여유롭게 스펙을 '고를' 형편이 아니었구나라는 쪽으로 생각을 고치게 되었고. 그보다는 메모리 대역폭이 GPU의 성능 수준을 근본적으로 결정하는 hard limit이라고 여기게 되었습니다.\n\n한편 위의 글에 다 담지 못한 트리비아 중에는 각 컴퓨팅 리소스간의 밸런스에 대한 저희의 소감이 있는데요, GPU 내 리소스의 밸런스를 가장 잘 잡는다는 측면에서 NVIDIA는 단연 독보적으로 보였습니다. 반면 AMD는 리소스간 밸런스 설계 측면에서 이해할 수 없는 결과를 내놓는 경우가 많았습니다. 메모리 대역폭에 비해 래스터라이저가 너무 부족했던 Fiji, 그리고 그 이후에는 래스터라이저와 메모리 대역폭의 불균형은 해소되었으나 범용 연산유닛의 비중이 항상 과하게 유지되었던 점 등을 꼽을 수 있겠네요. 다만 시간이 많이 지난 지금, 핵심은 그게 아니었다는 생각이 듭니다. 각 칩메이커의 설계는 각자가 이상적으로 생각하는 미래를 선반영하는 것이라 본다면 AMD는 범용 연산유닛이 항상 당대보다 더 많이 쓰일 미래를 염두에 둔 것이었겠지요.\n\n끝으로 딥러닝 시대의 초입이었던 2017년, 한 신문에 연재했던 글을 오랜만에 다시 생각해보며 글을 맺어봅니다.\n\n-\n\n\"인간이 다른 척색동물과 구분되는 가장 큰 차이는 ‘파충류의 뇌’ 바깥을 둘러싼 컴퓨팅 파워의 집합체 회백질이 고도로 발달해 있다는 점이다. 불행하게도 생물의 진화속도보다 컴퓨터의 발전속도가 월등히 빨랐던 탓에 오늘날 회백질은, 인류 문명이라는 그간 빛나는 성취에도 불구하고 지구상에 존재하는 가장 뛰어난 컴퓨터가 아니게 됐다. (중략)\n\n하지만 그것마저 본질은 아니다. 사실 오늘날 가장 발전된 로봇 기술은 방정식을 사람보다 빨리 풀거나 암호를 사람보다 빨리 맞추는 것에 있지 않다. 가장 뛰어난 과학자들이 모여 구현하는 것이라곤 어이없게도 갓 말을 뗀 어린이 수준의 인지능력이나 걸음마를 막 시작한 아이 정도의 보행능력이니. 인공지능의 미래가 ‘지능’ 그 자체는 아니라는 얘기다.\n\n인간에게 가장 쉬운 것이 컴퓨터에게는 가장 어렵고, 실은 인간 스스로도 ‘파충류의 뇌’의 관할인 반사신경과 불수의근의 도움이 없다면 도저히 불가능한 작업인 그것. 요컨대 인공지능의 미래가 ‘본능’에 있다는 하드웨어 제조사의 감각적이기 그지없는 작명은 결코 틀리지 않았을 뿐 아니라 실은 너무나 사실에 가까운 것이다. 회백질의 영역을 이미 넘어선 컴퓨터는 그 안으로 파고들어 인간의 본능을 모방하려 한다. (중략)\n\n오늘날 컴퓨터그래픽은 컴퓨터의 연산성능을 가장 많이 요구하는 분야 중 하나다. 공교롭게도 이 사상 역시 고성능의 단일 CPU에 의존적이던 당시의 기술을, 비록 개별 CPU의 성능은 낮더라도 이들을 여럿 묶어 성능의 총합을 높일 수 있다면 그에 비례해 그래픽 수준을 높일 수 있도록 하는 것이 핵심이었다. 이 프로젝트의 이름은 ‘맨틀(Mantle)’로 오늘날 컴퓨터 그래픽 표준인 다이렉트X 12(DirectX 12), 벌칸(Vulkan) 등에 계승됐다.\n\n한때 하드웨어 제조사들은 컴퓨터그래픽의 미래를 지구 깊은 곳 맨틀에서 찾으려 했다. 지금 그들은 우리 두뇌의 더 깊은 곳에서 컴퓨팅의 미래를 찾으려 한다. 공교롭게도 지구와 두뇌는 많은 면에서 닮았다. 산업혁명 이래 폭발적으로 지적 탐구가 행해졌음에도 아직까지도 지각 아래쪽은 인간의 눈으로 탐사된 바 없고, 여전히 회백질 아래 ‘파충류의 뇌’의 작동원리는 완벽히 드러나지 않았다.\n\n깊어질수록 닿기 어려운 지구, 파고들수록 지능과는 멀어지고 본능에 가까워지는 뇌. 이들은 불연속면으로 분절된 물리적 구조만큼이나 인류 지식의 지평이 닿아 있는 정도까지도 닮아 있다. 어쩌면 가장 익숙하고 가까운 대상의 가장 낯선 부분이기에 단연 인류 최후의 탐험지가 될 수밖에 없는 그곳. 거기에 컴퓨터의 미래가 있다.\"\n\n("
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/www.edaily.co.kr\/news\/read?newsId=01515366615990256",
                                "text": "https:\/\/www.edaily.co.kr\/news\/read?newsId=01515366615990256"
                            },
                            {
                                "type": "text",
                                "text": " )\n\n"
                            }
                        ]
                    }
                ]
            }
        ],
        "reactions": [
            {
                "name": "+1",
                "users": [
                    "U060BLVQYHW",
                    "U05VC7EEY5Q",
                    "U060LG0BT89"
                ],
                "count": 3
            }
        ]
    },
    {
        "user": "U065TBUL5EE",
        "type": "message",
        "ts": "1716861712.986589",
        "client_msg_id": "2EA424AF-7738-42C2-8E1A-19A02C9E0846",
        "text": "\n\nLLM에서 왜 quantization이 중요해졌는지에 대해 GPU의 발전역사와 연관되어 자세하게 설명된 글이 있어 공유해봅니다!\n\n\n\n(5\/27 Microsoft 사내 채널 릴레이 연재글입니다)\n\nAI 가속기의 과거, 현재, 미래\n\n1.\n \nMicrosoft에서 제공하는 많은 AI 서비스의 중심에는 VM으로 대표되는 Azure AI 인프라스트럭처가 있습니다. 특히 일반적인 범용 컴퓨팅용 VM에 더해 AI 작업에 요구되는 특화된 연산성능을 강화한 가속기를 탑재한 포트폴리오도 점점 다양해지고 있지요. 역사적으로(?) 컴퓨팅 성능을 향상시키기 위한 트렌드는 파편화된 연산장치를 하나로 통합하려는 힘과, 반대로 범용적으로 설계된 연산장치를 작업의 특성에 맞게 분화시키려는 힘이 교차하며 발전해 왔는데요, 이번 주의 learning snack에서는 이 두 트렌드가 어떻게 상호작용하며 오늘날의 AI 가속기들로 발전해 왔는지 &amp; 그 미래는 어떨지 짚어보도록 하겠습니다.\n \n오늘날 AI 작업에 쓰이는 가장 흔한 형태의 가속기는 GPU입니다. 그렇지만 처음부터 GPU를 AI 작업 - 구체적으로, \"연산\"에 활용하던 것은 아니었습니다. 프로그래머블하고 가장 범용적인 형태의 연산을 담당하는 장치의 위상은 꽤 오랜 기간 CPU가 독점하고 있었거든요. 사실 GPU라는 이름 자체가 대중화된 것도 컴퓨팅의 역사에서는 그리 오래되지 않았습니다. 3D 그래픽 화면을 그려내기 위해서 수행해야 하던 작업 중 물체의 변형과 광원 두 가지 작업을 처리할 수 있는 전용 하드웨어를 출시하며 CPU의 워크로드를 분담하게 되었고, 이러한 기하학(geometry)적 연산을 CPU로부터 이양하게 된 이 최초의 하드웨어를 개발사는 \"그래픽처리장치graphics processing unit(GPU)\"로 정의했습니다. 1999년 GeForce(\"Geometric Force\")의 등장입니다.\n \n이후 GeForce의 텍스처 파이프라인을 두 배로 늘려 초당 10억(giga)개의 텍스처 정보(texel)를 처리할 수 있게 한 GeForce 2 GTS (\"Giga TexelS\"), 프로그래머블한 쉐이더 파이프라인을 도입한 최초의 GPU인 GeForce 3, PCI-Express 인터페이스를 적용한 GeForce 4, 내부 부동소수점 벡터 포맷을 24비트에서 32비트로 확장한 GeForce FX를 거쳐 마침내 2006년 GeForce 8xxx 세대에서 '통합 쉐이더unified shader' 라는 기념비적 전환이 발표됩니다. 여기서 잠시, GPU 내부의 그래픽 처리 파이프라인을 간단히 짚고 넘어가겠습니다.\n \n \n2.\n \n3D 그래픽을 표현하기 위해 기하학적인 연산이 요구된다는 점을 앞에서 언급했는데, 구체적으로 GeForce 8xxx 이전까지의 GPU 내부에서는 물체의 꼭지점 정보를 처리하는 장치(\"vertex shader\")와 픽셀 정보를 처리하는 장치(\"pixel shader\")가 함께 존재했고 이에 따라 물체의 3D 메쉬 정보를 뒷 단계에 보내기까지의 로드 설계에 많은 노력이 들어갔습니다. (꼭지점을 다 그려도 픽셀 정보가 완료되기 전에는 다음 단계로 넘어갈 수 없고 그 반대도 마찬가지입니다) GeForce 8xxx 부터는 이것을 '통합 쉐이더unified shader'라는 범용적인 ALU로 치환하여, CPU 내의 FP 파이프라인과 유사한 연산장치를 다수 탑재하는 방향으로 전환한 것입니다. 이로써 후대의 GPU는 3D 메쉬 정보를 일원화된(\"unified\") 컴퓨팅 자원에 맡기게 되어 효율을 크게 높였고, 한편으로는 그래픽 작업이 아닌 다른 범용 연산에도 대응할 수 있게 되어 용도가 크게 확장되었습니다. NVIDIA는 이 unified shader를 \"Compute Unified Device Architecture(CUDA) core\"라 이름붙이고, CUDA core를 사용할 수 있는 라이브러리를 개발해 공개하기 시작했습니다. 오늘날 CUDA 생태계의 시작입니다.\n \n그럼에도 GPU에는 아직 여러 다양한 '특화 유닛'이 존재합니다. Unified shader가 3D 구조를 완성하더라도 이렇게 생성된 다각형의 각 면에 표면(texture) 정보를 입혀야(mapping) 하며, 표면 정보까지 입혀져 색칠된 3D 공간 정보가 완성되면 다시 이것을 매초, 매 프레임마다 우리가 모니터를 통해 보게 되는 2차원 평면화(rasterization)를 거쳐 납작하게 투영한 이미지로 그려내야 합니다. 이 나열된 과정들은 각각 텍스처 매핑 유닛(texture mapping unit)과 래스터라이저(rasterizer)라는 전용 유닛들이 담당하게 되며, 따라서 오늘날 GPU가 화면을 그려내는 과정을 아주 간단한 흐름도로 나타내면 아래와 같습니다.\n \n그래픽 메모리에서 정보 읽어오기 -&gt; 1. Unified shader의 연산(3D 구조 생성) -&gt; 2. 텍스처 매핑 유닛의 연산(표면 입히기) -&gt; 3. 래스터라이저의 연산(2D 평면화, 매 프레임 생성) \n여기에서 2번과 3번을 제외하면, unified shader를 활용해 GPU를 '연산' 용도만으로 활용하는 오늘날의 General-Purpose(범용) GPU(\"GPGPU\") 컴퓨팅의 흐름도가 됩니다. GPU의 성능을 GPU가 어떤 작업을 처리하는 데 소요되는 총 시간의 역수로 정의한다면, 다시 위의 각 과정에 소요되는 시간들의 합의 역수로 구할 수 있습니다. 그리고 우리가 학생 시절 배운 산술\/기하\/조화평균의 정의에 따라, 각 연산유닛의 성능 사이의 조화평균값(=역수의 평균)은 GPU 연산성능의 척도가 됩니다. 재미있는 점은 여기서 가장 전통적인 '그래픽' 성능에 기여율이 높은 항은 래스터라이저의 성능이고, '연산' 성능에 기여율이 높은 항은 역시 unified shader의 성능이 된다는 점입니다. 그럼 가장 기여율이 적은 항은 어디일까요? 바로 0번, 그래픽 메모리의 성능(대역폭)입니다.\n \n \n3.\n \n한편 GPGPU 컴퓨팅의 생태계가 확장되며 GPU가 처리하는 데이터의 포맷을 다양화하려는 목소리가 커졌습니다. 2000년대 초반까지만 하더라도 '컴퓨팅', 곧 슈퍼컴퓨터의 덕목은 '인간이 할 수 없는 고도로 정밀한 연산'을 수행하여 인간이 진출할 지평을 넓히는 데 있었습니다. 1차대전 전후 포탄의 탄도를 계산하던 연구는 오늘의 로켓 과학의 근간이 되었고 인류가 보통의 정밀도에서는 상상할 수도 없었을 천체물리학을 통한 스윙바이와 전인미답의 원거리 탐사를 가능하게 했습니다. 이를 위해 그동안의 부동소수점 연산 포맷의 표준이던 FP32(\"단정밀도single precision\"라는 이름부터 FP32가 당대의 컴퓨팅의 기준점임을 암시합니다)를 넘어, 두 배 넓은 포맷인 FP64(\"배정밀도double precision\")에 대응하는 연산 유닛이 탑재됩니다. NVIDIA의 Tesla 라는 연산 전용 GPU가 여기에 아주 좋은 성능을 제공했고, 그 흔적은 Tesla의 컨슈머 버전인 GeForce TITAN에 남아 있습니다. (TITAN을 사용해본 경험이 있으시다면, NVIDIA 제어판에서 \"Double Precision 활성화\" 옵션을 켜 보신 적이 있을 것 같습니다)\n \n그러나 2010년대 중반 이후 Convolutional 신경망(CNN), 딥 러닝이 GPGPU 컴퓨팅의 거대한 수요처가 되며 패러다임이 변화하기 시작합니다. 하나는 unified shader로부터 행렬연산에 최적화된 전용 연산유닛의 재분화가 그것이고, 다른 하나는 정밀도를 오히려 떨어뜨리려는 움직임입니다. 이후 행렬연산 전용 유닛인 \"텐서 코어tensor core\"가 도입되며 모든 범용연산을 unified shader가 담당하던 체제는 다시 막을 내렸고, FP32보다 간결한 포맷인 FP16(\"반정밀도half precision\"), 다시 그 절반인 INT8(8비트 정수), 그 절반인 INT4(4비트 정수) 등이 도입되었지요. 가장 큰 이유는, 인간의 뉴런을 모방하는 데 있어서 \"그렇게까지 정밀한 연산을 할 필요가 없다\" 다시 말해 \"낮은 정밀도의 연산을 더 많이 수행해 신경망의 중첩적인 수행을 모방하는 것\" 이 인간 뇌의 동작원리와 더 유사하고, 실제로 더 비슷한 결과값을 얻기 시작했기 때문입니다. 아주 거칠게 대입하면 FP64 연산 하나를 수행할 수 있는 규모의 칩 설계 자원(=트랜지스터 수)으로 FP32 두 개, FP16 네 개, INT8 여덟 개, INT4 열여섯 개를 수행할 수 있다면 개별 연산의 정밀도를 희생해서라도 2~16배 많은 연산을 수행해 CNN의 각 레이어를 더 많이 학습시키는 것이 훨씬 '인간적인' 결과를 만들 수 있었다는 이야기입니다.\n \n이렇게 정밀도를 반감시키며 총연산량(throughput)을 배증시키는 것은 오늘날 빅테크 다수가 채택한 전략입니다. NVIDIA의 B100 GPU가 H100보다 성능이 두 배 올랐다거나, Apple의 M4 SoC가 M3대비 NPU 성능이 두 배 오른 것은 모두 칩 설계 자원의 증가보다는 INT8 -&gt; INT4로의 이행에 의존하고 있습니다. 나아가 FP32 포맷을 구성하는 32개의 비트값에서, 유효숫자를 표현하는 23개의 비트와 지수를 표현하는 8개의 비트를 비슷한 비율로 줄인  FP16(유효숫자10+지수5)은 그 포맷이 간결해진 만큼 표현할 수 있는 수의 범위에도 차이가 있었는데(FP32는 10의 -8승부터 8승까지, FP16은 10의 -5승부터 5승까지), 아예 유효숫자를 대폭 줄이고 표현범위 자체는 그대로 유지하여(유효숫자7+지수8) 딥러닝에 특화된 새로운 포맷인 \"Brain\" BF16이 제안되기도 했습니다. 컴퓨팅의 패러다임이 \"인간이 할 수 없는 고도로 정밀한 연산을 하는 것\" 에서 \"인간의 뇌가 하듯 별로 정밀하지 않은 연산을, 대신 아주 많이 하는 것\" 으로 거대하게 방향을 틀어가고 있는 것입니다.\n \n심지어 지금도, INT4보다도 더 간소한 포맷으로의 탐구는 멈추지 않고 있습니다. 최근에는 1.58비트(2^1.58=3, -1, 0, 1의 3개 값을 표현하는 2의 지수값이 log_2(3)=1.5849이므로) 양자화, Boolean=1비트 (0, 1) 포맷 등 극도로 추상화된 포맷을 통해 모델을 압축하는 기법도 진지하게 논의되고 있는 시점입니다.\n \n \n4.\n \n왜 데이터 포맷을 끊임없이 간소화하는지 짚어 보자면 결국 파라미터 수가 기하급수적으로 늘어나는 오늘날의 LLM을 구동하기 위한 (특히 추론 작업에서) 그래픽 메모리 공간과 타협하기 위해서입니다. 앞서 GPU의 성능에 가장 기여도가 적은 요소가 그래픽 메모리의 대역폭이라고 말씀드렸기에, 스크롤의 압박을 이겨내고 이 글을 쭉 읽어내려오셨다면 위 두 명제가 모순이 아니냐고 생각하실 분이 계실 것 같네요. 여기는 한 가지 함정이 있습니다. 바로 성능에 기여하는 정도(통계학의 term으로 \"설명력\"이라고 합니다)는 다른 요소에 비해 메모리 대역폭이 가장 낮았던 것이 맞지만, GPU의 성능과 각 요소간의 상관관계correlation를 구해 보면 반대로 메모리 대역폭과의 상관관계가 가장 높게 나타나기 때문입니다. 심지어 제조사가 광고하는 GPU의 peak computing power(=unified shader 개수와 동작 클럭의 곱)보다도 correlation이 높습니다. 즉 메모리 대역폭은 그 세대의 GPU 성능수준을 원천적으로 결정짓는 변수이고, 여기서 도출된 GPU의 설계상 잠재력을 넘어 대역폭이 높아진다고 성능향상이 추가로 주어지지는 않지만 반대로 대역폭이 줄어들면 큰 성능하락을 보게 되는, 가장 근본적인 limiting factor라는 점입니다.\n \n또한 성능, 전력, 열이 고도로 집약된 데이터센터에서 \"성능\" 은 더 이상 독립된 변수로 존재할 수 없게 되었습니다. 데이터센터가 입주한 건물, 지역, 국가가 공급할 수 있는 전력의 총량과 열적 균형을 유지할 수 있는 열용량의 총량에 종속된 함수로서 \"성능\"이 도출되게 된 것이죠. 이러한 열설계전력에 가장 큰 병목으로 작용하는 요소는 unified shader가 소비하는 전력도, tensor core가 소비하는 전력도 아닌 메모리에서 1비트를 처리하는 데 필요한 전력입니다. 이를 타개하기 위해 데이터 포맷 간소화를 통한 경량화, 모델 자체의 파라미터 소거를 통한 압축(pruning이라는 기법을 이용하는데 글이 이미 산으로 가고 있기 때문에 여기서는 더 다루지 않겠습니다), 그리고 하드웨어 레벨에서는 메모리 계층 구조(memory hierarchy) 자체를 개혁하는 방법 등이 있습니다. 대표적인 세번째 예시는 캐시메모리의 용량을 칩 전체 규모 대비 크게 늘린 Apple의 SoC들, 그리고 극단적으로 모든 메모리계층을 SRAM으로 단일화한 GROQ 등의 사례가 있으나 칩의 비용최적화 측면에서 대중적인 해결책으로 보기 어렵다는 것이 아직까지의 연구자들의 시각입니다. 결국은 주어진 조건 아래 메모리를 최대한 알뜰하게 사용하는 것이 AI의 성능을 높이는 길인 것이지요.\n \n주로 제조사의 접근법에서는 일반론적인 표준, 데이터 포맷 자체를 재정의하며 모든 모델\/서비스에 범용적으로 적용될 수 있는 방향으로 경량화 = 메모리(VRAM) 효율 = GPU 효율 = 궁극적으로 데이터센터의 전력효율을 유도하고 있으나, LLM을 개발하고 서비스를 운용하는 회사들은 조금 시각이 다릅니다. NVIDIA는 Blackwell을 발표하며 pruning 기법 중 하나로 인접한 파라미터의 중복값을 기계적으로 소거하는 sparsity 알고리즘을 공개했으나 (제조사의 view), 이렇게 압축하면 메모리 용량을 덜 먹는데도 성능이 오히려 떨어집니다 (서비스사의 view). 서비스사가 연구하는 인공신경망의 성능을 높일 수 있는 방법론은 아직까지 정형성이 도출되지 않아 하드웨어로 구현하기가 어렵고, 제조사가 제안하는 정형적인 방법론은 인공신경망의 성능향상으로 이어지지 않는 교착상태에 있는 것이죠. 물론 여기서 돌파구를 찾기 위한 서비스사-제조사간의 합종연횡 역시 이어지고 있습니다. Microsoft의 자체 가속기 Maia도 이러한 흐름에 한 발을 내딛는 것이길, 향후에도 더 흥미로운 자체 AI 인프라스트럭처를 구축해 고객들에게 소개할 수 있기를 바라며, 이번 learning snack을 맺어 봅니다. (*회사 연재분은 여기서 끝)\n\n-\n\n닥터몰라를 지켜봐 오신 분들이라면 금방 떠올리실 수 있는 소재가 사용되었는데, 바로 GPU 성능 시뮬레이터가 그것입니다. GPU 시뮬레이터는 주어진 여러 독립변수를 조합해 다시 외부에서 주어진 성능이라는 종속변수를 이끌어내는 모델이었는데 항의 개수가 한정된 다항식의 특성상 오버피팅을 피할 수 없었습니다. 메모리 대역폭의 성능에의 기여도와 상관관계의 역설은 당시 실험을 통해 얻은 재미있는 트리비아 중 하나였습니다.\n\n시뮬레이션 수식 내의 가중치가 가장 적음에도, 종속변수인 성능과의 상관관계가 가장 크다는 역설.\n\n처음에는 칩메이커들이 내부적으로 시뮬레이션을 돌려 그에 가장 알맞은 메모리 스펙을 '고르는' 것이 아닐까 생각했으나, 딥러닝의 시대에 접어들며 메모리의 한계에 괴로워하는(?) 칩메이커들을 보니 그리 여유롭게 스펙을 '고를' 형편이 아니었구나라는 쪽으로 생각을 고치게 되었고. 그보다는 메모리 대역폭이 GPU의 성능 수준을 근본적으로 결정하는 hard limit이라고 여기게 되었습니다.\n\n한편 위의 글에 다 담지 못한 트리비아 중에는 각 컴퓨팅 리소스간의 밸런스에 대한 저희의 소감이 있는데요, GPU 내 리소스의 밸런스를 가장 잘 잡는다는 측면에서 NVIDIA는 단연 독보적으로 보였습니다. 반면 AMD는 리소스간 밸런스 설계 측면에서 이해할 수 없는 결과를 내놓는 경우가 많았습니다. 메모리 대역폭에 비해 래스터라이저가 너무 부족했던 Fiji, 그리고 그 이후에는 래스터라이저와 메모리 대역폭의 불균형은 해소되었으나 범용 연산유닛의 비중이 항상 과하게 유지되었던 점 등을 꼽을 수 있겠네요. 다만 시간이 많이 지난 지금, 핵심은 그게 아니었다는 생각이 듭니다. 각 칩메이커의 설계는 각자가 이상적으로 생각하는 미래를 선반영하는 것이라 본다면 AMD는 범용 연산유닛이 항상 당대보다 더 많이 쓰일 미래를 염두에 둔 것이었겠지요.\n\n끝으로 딥러닝 시대의 초입이었던 2017년, 한 신문에 연재했던 글을 오랜만에 다시 생각해보며 글을 맺어봅니다.\n\n-\n\n\"인간이 다른 척색동물과 구분되는 가장 큰 차이는 ‘파충류의 뇌’ 바깥을 둘러싼 컴퓨팅 파워의 집합체 회백질이 고도로 발달해 있다는 점이다. 불행하게도 생물의 진화속도보다 컴퓨터의 발전속도가 월등히 빨랐던 탓에 오늘날 회백질은, 인류 문명이라는 그간 빛나는 성취에도 불구하고 지구상에 존재하는 가장 뛰어난 컴퓨터가 아니게 됐다. (중략)\n\n하지만 그것마저 본질은 아니다. 사실 오늘날 가장 발전된 로봇 기술은 방정식을 사람보다 빨리 풀거나 암호를 사람보다 빨리 맞추는 것에 있지 않다. 가장 뛰어난 과학자들이 모여 구현하는 것이라곤 어이없게도 갓 말을 뗀 어린이 수준의 인지능력이나 걸음마를 막 시작한 아이 정도의 보행능력이니. 인공지능의 미래가 ‘지능’ 그 자체는 아니라는 얘기다.\n\n인간에게 가장 쉬운 것이 컴퓨터에게는 가장 어렵고, 실은 인간 스스로도 ‘파충류의 뇌’의 관할인 반사신경과 불수의근의 도움이 없다면 도저히 불가능한 작업인 그것. 요컨대 인공지능의 미래가 ‘본능’에 있다는 하드웨어 제조사의 감각적이기 그지없는 작명은 결코 틀리지 않았을 뿐 아니라 실은 너무나 사실에 가까운 것이다. 회백질의 영역을 이미 넘어선 컴퓨터는 그 안으로 파고들어 인간의 본능을 모방하려 한다. (중략)\n\n오늘날 컴퓨터그래픽은 컴퓨터의 연산성능을 가장 많이 요구하는 분야 중 하나다. 공교롭게도 이 사상 역시 고성능의 단일 CPU에 의존적이던 당시의 기술을, 비록 개별 CPU의 성능은 낮더라도 이들을 여럿 묶어 성능의 총합을 높일 수 있다면 그에 비례해 그래픽 수준을 높일 수 있도록 하는 것이 핵심이었다. 이 프로젝트의 이름은 ‘맨틀(Mantle)’로 오늘날 컴퓨터 그래픽 표준인 다이렉트X 12(DirectX 12), 벌칸(Vulkan) 등에 계승됐다.\n\n한때 하드웨어 제조사들은 컴퓨터그래픽의 미래를 지구 깊은 곳 맨틀에서 찾으려 했다. 지금 그들은 우리 두뇌의 더 깊은 곳에서 컴퓨팅의 미래를 찾으려 한다. 공교롭게도 지구와 두뇌는 많은 면에서 닮았다. 산업혁명 이래 폭발적으로 지적 탐구가 행해졌음에도 아직까지도 지각 아래쪽은 인간의 눈으로 탐사된 바 없고, 여전히 회백질 아래 ‘파충류의 뇌’의 작동원리는 완벽히 드러나지 않았다.\n\n깊어질수록 닿기 어려운 지구, 파고들수록 지능과는 멀어지고 본능에 가까워지는 뇌. 이들은 불연속면으로 분절된 물리적 구조만큼이나 인류 지식의 지평이 닿아 있는 정도까지도 닮아 있다. 어쩌면 가장 익숙하고 가까운 대상의 가장 낯선 부분이기에 단연 인류 최후의 탐험지가 될 수밖에 없는 그곳. 거기에 컴퓨터의 미래가 있다.\"\n\n(<https:\/\/www.edaily.co.kr\/news\/read?newsId=01515366615990256|https:\/\/www.edaily.co.kr\/news\/read?newsId=01515366615990256> )\n\n",
        "team": "T05UGFFGL07",
        "user_team": "T05UGFFGL07",
        "source_team": "T05UGFFGL07",
        "user_profile": {
            "avatar_hash": "34f110a5edc4",
            "image_72": "https:\/\/avatars.slack-edge.com\/2023-11-14\/6197301711475_34f110a5edc435398e63_72.png",
            "first_name": "김용담",
            "real_name": "김용담",
            "display_name": "김용담 강사",
            "team": "T05UGFFGL07",
            "name": "codingiscoffee",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "LNzYG",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "\n"
                            },
                            {
                                "type": "text",
                                "text": "\nLLM에서 왜 quantization이 중요해졌는지에 대해 GPU의 발전역사와 연관되어 자세하게 설명된 글이 있어 공유해봅니다!\n\n\n\n(5\/27 Microsoft 사내 채널 릴레이 연재글입니다)\n\nAI 가속기의 과거, 현재, 미래\n\n1.\n \nMicrosoft에서 제공하는 많은 AI 서비스의 중심에는 VM으로 대표되는 Azure AI 인프라스트럭처가 있습니다. 특히 일반적인 범용 컴퓨팅용 VM에 더해 AI 작업에 요구되는 특화된 연산성능을 강화한 가속기를 탑재한 포트폴리오도 점점 다양해지고 있지요. 역사적으로(?) 컴퓨팅 성능을 향상시키기 위한 트렌드는 파편화된 연산장치를 하나로 통합하려는 힘과, 반대로 범용적으로 설계된 연산장치를 작업의 특성에 맞게 분화시키려는 힘이 교차하며 발전해 왔는데요, 이번 주의 learning snack에서는 이 두 트렌드가 어떻게 상호작용하며 오늘날의 AI 가속기들로 발전해 왔는지 & 그 미래는 어떨지 짚어보도록 하겠습니다.\n \n오늘날 AI 작업에 쓰이는 가장 흔한 형태의 가속기는 GPU입니다. 그렇지만 처음부터 GPU를 AI 작업 - 구체적으로, \"연산\"에 활용하던 것은 아니었습니다. 프로그래머블하고 가장 범용적인 형태의 연산을 담당하는 장치의 위상은 꽤 오랜 기간 CPU가 독점하고 있었거든요. 사실 GPU라는 이름 자체가 대중화된 것도 컴퓨팅의 역사에서는 그리 오래되지 않았습니다. 3D 그래픽 화면을 그려내기 위해서 수행해야 하던 작업 중 물체의 변형과 광원 두 가지 작업을 처리할 수 있는 전용 하드웨어를 출시하며 CPU의 워크로드를 분담하게 되었고, 이러한 기하학(geometry)적 연산을 CPU로부터 이양하게 된 이 최초의 하드웨어를 개발사는 \"그래픽처리장치graphics processing unit(GPU)\"로 정의했습니다. 1999년 GeForce(\"Geometric Force\")의 등장입니다.\n \n이후 GeForce의 텍스처 파이프라인을 두 배로 늘려 초당 10억(giga)개의 텍스처 정보(texel)를 처리할 수 있게 한 GeForce 2 GTS (\"Giga TexelS\"), 프로그래머블한 쉐이더 파이프라인을 도입한 최초의 GPU인 GeForce 3, PCI-Express 인터페이스를 적용한 GeForce 4, 내부 부동소수점 벡터 포맷을 24비트에서 32비트로 확장한 GeForce FX를 거쳐 마침내 2006년 GeForce 8xxx 세대에서 '통합 쉐이더unified shader' 라는 기념비적 전환이 발표됩니다. 여기서 잠시, GPU 내부의 그래픽 처리 파이프라인을 간단히 짚고 넘어가겠습니다.\n \n \n2.\n \n3D 그래픽을 표현하기 위해 기하학적인 연산이 요구된다는 점을 앞에서 언급했는데, 구체적으로 GeForce 8xxx 이전까지의 GPU 내부에서는 물체의 꼭지점 정보를 처리하는 장치(\"vertex shader\")와 픽셀 정보를 처리하는 장치(\"pixel shader\")가 함께 존재했고 이에 따라 물체의 3D 메쉬 정보를 뒷 단계에 보내기까지의 로드 설계에 많은 노력이 들어갔습니다. (꼭지점을 다 그려도 픽셀 정보가 완료되기 전에는 다음 단계로 넘어갈 수 없고 그 반대도 마찬가지입니다) GeForce 8xxx 부터는 이것을 '통합 쉐이더unified shader'라는 범용적인 ALU로 치환하여, CPU 내의 FP 파이프라인과 유사한 연산장치를 다수 탑재하는 방향으로 전환한 것입니다. 이로써 후대의 GPU는 3D 메쉬 정보를 일원화된(\"unified\") 컴퓨팅 자원에 맡기게 되어 효율을 크게 높였고, 한편으로는 그래픽 작업이 아닌 다른 범용 연산에도 대응할 수 있게 되어 용도가 크게 확장되었습니다. NVIDIA는 이 unified shader를 \"Compute Unified Device Architecture(CUDA) core\"라 이름붙이고, CUDA core를 사용할 수 있는 라이브러리를 개발해 공개하기 시작했습니다. 오늘날 CUDA 생태계의 시작입니다.\n \n그럼에도 GPU에는 아직 여러 다양한 '특화 유닛'이 존재합니다. Unified shader가 3D 구조를 완성하더라도 이렇게 생성된 다각형의 각 면에 표면(texture) 정보를 입혀야(mapping) 하며, 표면 정보까지 입혀져 색칠된 3D 공간 정보가 완성되면 다시 이것을 매초, 매 프레임마다 우리가 모니터를 통해 보게 되는 2차원 평면화(rasterization)를 거쳐 납작하게 투영한 이미지로 그려내야 합니다. 이 나열된 과정들은 각각 텍스처 매핑 유닛(texture mapping unit)과 래스터라이저(rasterizer)라는 전용 유닛들이 담당하게 되며, 따라서 오늘날 GPU가 화면을 그려내는 과정을 아주 간단한 흐름도로 나타내면 아래와 같습니다.\n \n그래픽 메모리에서 정보 읽어오기 -> 1. Unified shader의 연산(3D 구조 생성) -> 2. 텍스처 매핑 유닛의 연산(표면 입히기) -> 3. 래스터라이저의 연산(2D 평면화, 매 프레임 생성) \n여기에서 2번과 3번을 제외하면, unified shader를 활용해 GPU를 '연산' 용도만으로 활용하는 오늘날의 General-Purpose(범용) GPU(\"GPGPU\") 컴퓨팅의 흐름도가 됩니다. GPU의 성능을 GPU가 어떤 작업을 처리하는 데 소요되는 총 시간의 역수로 정의한다면, 다시 위의 각 과정에 소요되는 시간들의 합의 역수로 구할 수 있습니다. 그리고 우리가 학생 시절 배운 산술\/기하\/조화평균의 정의에 따라, 각 연산유닛의 성능 사이의 조화평균값(=역수의 평균)은 GPU 연산성능의 척도가 됩니다. 재미있는 점은 여기서 가장 전통적인 '그래픽' 성능에 기여율이 높은 항은 래스터라이저의 성능이고, '연산' 성능에 기여율이 높은 항은 역시 unified shader의 성능이 된다는 점입니다. 그럼 가장 기여율이 적은 항은 어디일까요? 바로 0번, 그래픽 메모리의 성능(대역폭)입니다.\n \n \n3.\n \n한편 GPGPU 컴퓨팅의 생태계가 확장되며 GPU가 처리하는 데이터의 포맷을 다양화하려는 목소리가 커졌습니다. 2000년대 초반까지만 하더라도 '컴퓨팅', 곧 슈퍼컴퓨터의 덕목은 '인간이 할 수 없는 고도로 정밀한 연산'을 수행하여 인간이 진출할 지평을 넓히는 데 있었습니다. 1차대전 전후 포탄의 탄도를 계산하던 연구는 오늘의 로켓 과학의 근간이 되었고 인류가 보통의 정밀도에서는 상상할 수도 없었을 천체물리학을 통한 스윙바이와 전인미답의 원거리 탐사를 가능하게 했습니다. 이를 위해 그동안의 부동소수점 연산 포맷의 표준이던 FP32(\"단정밀도single precision\"라는 이름부터 FP32가 당대의 컴퓨팅의 기준점임을 암시합니다)를 넘어, 두 배 넓은 포맷인 FP64(\"배정밀도double precision\")에 대응하는 연산 유닛이 탑재됩니다. NVIDIA의 Tesla 라는 연산 전용 GPU가 여기에 아주 좋은 성능을 제공했고, 그 흔적은 Tesla의 컨슈머 버전인 GeForce TITAN에 남아 있습니다. (TITAN을 사용해본 경험이 있으시다면, NVIDIA 제어판에서 \"Double Precision 활성화\" 옵션을 켜 보신 적이 있을 것 같습니다)\n \n그러나 2010년대 중반 이후 Convolutional 신경망(CNN), 딥 러닝이 GPGPU 컴퓨팅의 거대한 수요처가 되며 패러다임이 변화하기 시작합니다. 하나는 unified shader로부터 행렬연산에 최적화된 전용 연산유닛의 재분화가 그것이고, 다른 하나는 정밀도를 오히려 떨어뜨리려는 움직임입니다. 이후 행렬연산 전용 유닛인 \"텐서 코어tensor core\"가 도입되며 모든 범용연산을 unified shader가 담당하던 체제는 다시 막을 내렸고, FP32보다 간결한 포맷인 FP16(\"반정밀도half precision\"), 다시 그 절반인 INT8(8비트 정수), 그 절반인 INT4(4비트 정수) 등이 도입되었지요. 가장 큰 이유는, 인간의 뉴런을 모방하는 데 있어서 \"그렇게까지 정밀한 연산을 할 필요가 없다\" 다시 말해 \"낮은 정밀도의 연산을 더 많이 수행해 신경망의 중첩적인 수행을 모방하는 것\" 이 인간 뇌의 동작원리와 더 유사하고, 실제로 더 비슷한 결과값을 얻기 시작했기 때문입니다. 아주 거칠게 대입하면 FP64 연산 하나를 수행할 수 있는 규모의 칩 설계 자원(=트랜지스터 수)으로 FP32 두 개, FP16 네 개, INT8 여덟 개, INT4 열여섯 개를 수행할 수 있다면 개별 연산의 정밀도를 희생해서라도 2~16배 많은 연산을 수행해 CNN의 각 레이어를 더 많이 학습시키는 것이 훨씬 '인간적인' 결과를 만들 수 있었다는 이야기입니다.\n \n이렇게 정밀도를 반감시키며 총연산량(throughput)을 배증시키는 것은 오늘날 빅테크 다수가 채택한 전략입니다. NVIDIA의 B100 GPU가 H100보다 성능이 두 배 올랐다거나, Apple의 M4 SoC가 M3대비 NPU 성능이 두 배 오른 것은 모두 칩 설계 자원의 증가보다는 INT8 -> INT4로의 이행에 의존하고 있습니다. 나아가 FP32 포맷을 구성하는 32개의 비트값에서, 유효숫자를 표현하는 23개의 비트와 지수를 표현하는 8개의 비트를 비슷한 비율로 줄인  FP16(유효숫자10+지수5)은 그 포맷이 간결해진 만큼 표현할 수 있는 수의 범위에도 차이가 있었는데(FP32는 10의 -8승부터 8승까지, FP16은 10의 -5승부터 5승까지), 아예 유효숫자를 대폭 줄이고 표현범위 자체는 그대로 유지하여(유효숫자7+지수8) 딥러닝에 특화된 새로운 포맷인 \"Brain\" BF16이 제안되기도 했습니다. 컴퓨팅의 패러다임이 \"인간이 할 수 없는 고도로 정밀한 연산을 하는 것\" 에서 \"인간의 뇌가 하듯 별로 정밀하지 않은 연산을, 대신 아주 많이 하는 것\" 으로 거대하게 방향을 틀어가고 있는 것입니다.\n \n심지어 지금도, INT4보다도 더 간소한 포맷으로의 탐구는 멈추지 않고 있습니다. 최근에는 1.58비트(2^1.58=3, -1, 0, 1의 3개 값을 표현하는 2의 지수값이 log_2(3)=1.5849이므로) 양자화, Boolean=1비트 (0, 1) 포맷 등 극도로 추상화된 포맷을 통해 모델을 압축하는 기법도 진지하게 논의되고 있는 시점입니다.\n \n \n4.\n \n왜 데이터 포맷을 끊임없이 간소화하는지 짚어 보자면 결국 파라미터 수가 기하급수적으로 늘어나는 오늘날의 LLM을 구동하기 위한 (특히 추론 작업에서) 그래픽 메모리 공간과 타협하기 위해서입니다. 앞서 GPU의 성능에 가장 기여도가 적은 요소가 그래픽 메모리의 대역폭이라고 말씀드렸기에, 스크롤의 압박을 이겨내고 이 글을 쭉 읽어내려오셨다면 위 두 명제가 모순이 아니냐고 생각하실 분이 계실 것 같네요. 여기는 한 가지 함정이 있습니다. 바로 성능에 기여하는 정도(통계학의 term으로 \"설명력\"이라고 합니다)는 다른 요소에 비해 메모리 대역폭이 가장 낮았던 것이 맞지만, GPU의 성능과 각 요소간의 상관관계correlation를 구해 보면 반대로 메모리 대역폭과의 상관관계가 가장 높게 나타나기 때문입니다. 심지어 제조사가 광고하는 GPU의 peak computing power(=unified shader 개수와 동작 클럭의 곱)보다도 correlation이 높습니다. 즉 메모리 대역폭은 그 세대의 GPU 성능수준을 원천적으로 결정짓는 변수이고, 여기서 도출된 GPU의 설계상 잠재력을 넘어 대역폭이 높아진다고 성능향상이 추가로 주어지지는 않지만 반대로 대역폭이 줄어들면 큰 성능하락을 보게 되는, 가장 근본적인 limiting factor라는 점입니다.\n \n또한 성능, 전력, 열이 고도로 집약된 데이터센터에서 \"성능\" 은 더 이상 독립된 변수로 존재할 수 없게 되었습니다. 데이터센터가 입주한 건물, 지역, 국가가 공급할 수 있는 전력의 총량과 열적 균형을 유지할 수 있는 열용량의 총량에 종속된 함수로서 \"성능\"이 도출되게 된 것이죠. 이러한 열설계전력에 가장 큰 병목으로 작용하는 요소는 unified shader가 소비하는 전력도, tensor core가 소비하는 전력도 아닌 메모리에서 1비트를 처리하는 데 필요한 전력입니다. 이를 타개하기 위해 데이터 포맷 간소화를 통한 경량화, 모델 자체의 파라미터 소거를 통한 압축(pruning이라는 기법을 이용하는데 글이 이미 산으로 가고 있기 때문에 여기서는 더 다루지 않겠습니다), 그리고 하드웨어 레벨에서는 메모리 계층 구조(memory hierarchy) 자체를 개혁하는 방법 등이 있습니다. 대표적인 세번째 예시는 캐시메모리의 용량을 칩 전체 규모 대비 크게 늘린 Apple의 SoC들, 그리고 극단적으로 모든 메모리계층을 SRAM으로 단일화한 GROQ 등의 사례가 있으나 칩의 비용최적화 측면에서 대중적인 해결책으로 보기 어렵다는 것이 아직까지의 연구자들의 시각입니다. 결국은 주어진 조건 아래 메모리를 최대한 알뜰하게 사용하는 것이 AI의 성능을 높이는 길인 것이지요.\n \n주로 제조사의 접근법에서는 일반론적인 표준, 데이터 포맷 자체를 재정의하며 모든 모델\/서비스에 범용적으로 적용될 수 있는 방향으로 경량화 = 메모리(VRAM) 효율 = GPU 효율 = 궁극적으로 데이터센터의 전력효율을 유도하고 있으나, LLM을 개발하고 서비스를 운용하는 회사들은 조금 시각이 다릅니다. NVIDIA는 Blackwell을 발표하며 pruning 기법 중 하나로 인접한 파라미터의 중복값을 기계적으로 소거하는 sparsity 알고리즘을 공개했으나 (제조사의 view), 이렇게 압축하면 메모리 용량을 덜 먹는데도 성능이 오히려 떨어집니다 (서비스사의 view). 서비스사가 연구하는 인공신경망의 성능을 높일 수 있는 방법론은 아직까지 정형성이 도출되지 않아 하드웨어로 구현하기가 어렵고, 제조사가 제안하는 정형적인 방법론은 인공신경망의 성능향상으로 이어지지 않는 교착상태에 있는 것이죠. 물론 여기서 돌파구를 찾기 위한 서비스사-제조사간의 합종연횡 역시 이어지고 있습니다. Microsoft의 자체 가속기 Maia도 이러한 흐름에 한 발을 내딛는 것이길, 향후에도 더 흥미로운 자체 AI 인프라스트럭처를 구축해 고객들에게 소개할 수 있기를 바라며, 이번 learning snack을 맺어 봅니다. (*회사 연재분은 여기서 끝)\n\n-\n\n닥터몰라를 지켜봐 오신 분들이라면 금방 떠올리실 수 있는 소재가 사용되었는데, 바로 GPU 성능 시뮬레이터가 그것입니다. GPU 시뮬레이터는 주어진 여러 독립변수를 조합해 다시 외부에서 주어진 성능이라는 종속변수를 이끌어내는 모델이었는데 항의 개수가 한정된 다항식의 특성상 오버피팅을 피할 수 없었습니다. 메모리 대역폭의 성능에의 기여도와 상관관계의 역설은 당시 실험을 통해 얻은 재미있는 트리비아 중 하나였습니다.\n\n시뮬레이션 수식 내의 가중치가 가장 적음에도, 종속변수인 성능과의 상관관계가 가장 크다는 역설.\n\n처음에는 칩메이커들이 내부적으로 시뮬레이션을 돌려 그에 가장 알맞은 메모리 스펙을 '고르는' 것이 아닐까 생각했으나, 딥러닝의 시대에 접어들며 메모리의 한계에 괴로워하는(?) 칩메이커들을 보니 그리 여유롭게 스펙을 '고를' 형편이 아니었구나라는 쪽으로 생각을 고치게 되었고. 그보다는 메모리 대역폭이 GPU의 성능 수준을 근본적으로 결정하는 hard limit이라고 여기게 되었습니다.\n\n한편 위의 글에 다 담지 못한 트리비아 중에는 각 컴퓨팅 리소스간의 밸런스에 대한 저희의 소감이 있는데요, GPU 내 리소스의 밸런스를 가장 잘 잡는다는 측면에서 NVIDIA는 단연 독보적으로 보였습니다. 반면 AMD는 리소스간 밸런스 설계 측면에서 이해할 수 없는 결과를 내놓는 경우가 많았습니다. 메모리 대역폭에 비해 래스터라이저가 너무 부족했던 Fiji, 그리고 그 이후에는 래스터라이저와 메모리 대역폭의 불균형은 해소되었으나 범용 연산유닛의 비중이 항상 과하게 유지되었던 점 등을 꼽을 수 있겠네요. 다만 시간이 많이 지난 지금, 핵심은 그게 아니었다는 생각이 듭니다. 각 칩메이커의 설계는 각자가 이상적으로 생각하는 미래를 선반영하는 것이라 본다면 AMD는 범용 연산유닛이 항상 당대보다 더 많이 쓰일 미래를 염두에 둔 것이었겠지요.\n\n끝으로 딥러닝 시대의 초입이었던 2017년, 한 신문에 연재했던 글을 오랜만에 다시 생각해보며 글을 맺어봅니다.\n\n-\n\n\"인간이 다른 척색동물과 구분되는 가장 큰 차이는 ‘파충류의 뇌’ 바깥을 둘러싼 컴퓨팅 파워의 집합체 회백질이 고도로 발달해 있다는 점이다. 불행하게도 생물의 진화속도보다 컴퓨터의 발전속도가 월등히 빨랐던 탓에 오늘날 회백질은, 인류 문명이라는 그간 빛나는 성취에도 불구하고 지구상에 존재하는 가장 뛰어난 컴퓨터가 아니게 됐다. (중략)\n\n하지만 그것마저 본질은 아니다. 사실 오늘날 가장 발전된 로봇 기술은 방정식을 사람보다 빨리 풀거나 암호를 사람보다 빨리 맞추는 것에 있지 않다. 가장 뛰어난 과학자들이 모여 구현하는 것이라곤 어이없게도 갓 말을 뗀 어린이 수준의 인지능력이나 걸음마를 막 시작한 아이 정도의 보행능력이니. 인공지능의 미래가 ‘지능’ 그 자체는 아니라는 얘기다.\n\n인간에게 가장 쉬운 것이 컴퓨터에게는 가장 어렵고, 실은 인간 스스로도 ‘파충류의 뇌’의 관할인 반사신경과 불수의근의 도움이 없다면 도저히 불가능한 작업인 그것. 요컨대 인공지능의 미래가 ‘본능’에 있다는 하드웨어 제조사의 감각적이기 그지없는 작명은 결코 틀리지 않았을 뿐 아니라 실은 너무나 사실에 가까운 것이다. 회백질의 영역을 이미 넘어선 컴퓨터는 그 안으로 파고들어 인간의 본능을 모방하려 한다. (중략)\n\n오늘날 컴퓨터그래픽은 컴퓨터의 연산성능을 가장 많이 요구하는 분야 중 하나다. 공교롭게도 이 사상 역시 고성능의 단일 CPU에 의존적이던 당시의 기술을, 비록 개별 CPU의 성능은 낮더라도 이들을 여럿 묶어 성능의 총합을 높일 수 있다면 그에 비례해 그래픽 수준을 높일 수 있도록 하는 것이 핵심이었다. 이 프로젝트의 이름은 ‘맨틀(Mantle)’로 오늘날 컴퓨터 그래픽 표준인 다이렉트X 12(DirectX 12), 벌칸(Vulkan) 등에 계승됐다.\n\n한때 하드웨어 제조사들은 컴퓨터그래픽의 미래를 지구 깊은 곳 맨틀에서 찾으려 했다. 지금 그들은 우리 두뇌의 더 깊은 곳에서 컴퓨팅의 미래를 찾으려 한다. 공교롭게도 지구와 두뇌는 많은 면에서 닮았다. 산업혁명 이래 폭발적으로 지적 탐구가 행해졌음에도 아직까지도 지각 아래쪽은 인간의 눈으로 탐사된 바 없고, 여전히 회백질 아래 ‘파충류의 뇌’의 작동원리는 완벽히 드러나지 않았다.\n\n깊어질수록 닿기 어려운 지구, 파고들수록 지능과는 멀어지고 본능에 가까워지는 뇌. 이들은 불연속면으로 분절된 물리적 구조만큼이나 인류 지식의 지평이 닿아 있는 정도까지도 닮아 있다. 어쩌면 가장 익숙하고 가까운 대상의 가장 낯선 부분이기에 단연 인류 최후의 탐험지가 될 수밖에 없는 그곳. 거기에 컴퓨터의 미래가 있다.\"\n\n("
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/www.edaily.co.kr\/news\/read?newsId=01515366615990256",
                                "text": "https:\/\/www.edaily.co.kr\/news\/read?newsId=01515366615990256"
                            },
                            {
                                "type": "text",
                                "text": " )\n\n"
                            }
                        ]
                    }
                ]
            }
        ],
        "reactions": [
            {
                "name": "+1",
                "users": [
                    "U060LG0BT89",
                    "U05VBQMH3D3"
                ],
                "count": 2
            }
        ]
    }
]